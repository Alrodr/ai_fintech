---
title: "Data Processing: Fundamentals II"
description: "Data Processing. Concepts at a high level"
lead: "Data Processing. Concepts at a high level"
date: 2021-05-01
lastmod: 2021-05-01
draft: false
images: []
menu:
  docs:
    parent: "Data Processing: Fundamentals I"
weight: 110
toc: true
---

In this page, we are going to learn all about data processing pipelines. Pipelines are frequently built by data engineers to enable data to get from A to B, or maybe that should be from A to E or even F as there are always a few stages in between ðŸ™ƒ. Pipelines are a fundamental part of any data processing system. At a high level, there are four main stages to a data pipeline, ingestion, storage, processing, and visualization. We are going to dedicate an entire chapter to storage. So for this time, let's take a look at some concepts for the other 3. Let's start with data ingestion. 

## Data ingestion

This is the process by which we move data into our data processing system. The data itself may come from numerous sources and be in many different formats, the final destination of our data processing system, maybe a data warehouse or a data lake as discussed in the previous page. But ingestion is always the first stage in getting it there. As we also learned in our concepts lecture, data is usually ingested in 1 of 2 ways. As a batch, where we take a large amount of data, define the parameters for its ingestion and load it all in one go, or with streaming ingestion, when new data is continually being loaded into the system. And again, it's worth noting, that some systems simply do this with very small batches rather than individual items of data. This is all still very abstract at the moment. So to help you conceptualize this part of the data pipeline, let's have a quick think about the types of data we might want to ingest. 

We could be moving a large amount of archive data into a larger data analytic system. For example, importing relational data into BigQuery. We could be collecting logs from multiple systems in our stack so that we can aggregate them and analyze them later. Or perhaps we are streaming events from real-time devices. This could be anything from medical devices to wind turbines, to your family car. And these are just 3 suggestions. In reality, there are hundreds more data sources to ingest from. There are several technical challenges to bear in mind when designing the ingestion stage of your data pipeline. Big data by its very nature is big. So, you need to consider network design, the technical mechanisms for moving data, be it HTTP, FTP, or something else, and how these things will affect the time it takes to load your data. You need to choose the correct compute and storage options for the type of data you are ingesting. **Otherwise, you may end up with a solution that may be too expensive or too slow**. 

Your choice of batch or streaming ingestion will also affect these outcomes. As we progress, we will learn more about designing these systems and help you make the right choices here. But there are also some non-technical challenges that you should bear in mind. **The data you are ingesting should have value**. It should be useful to the business storing or using it. Otherwise, it's just wasting money to store data for its own sake. Moving data can make it vulnerable. This means you need to consider the security implications of your data processing design to reduce the possibility of a data breach. You may also be working in an industry where compliance and regulation apply, in which case, data breaches could be disastrous and regulations may mandate additional transformational steps to take with your data. Don't worry. We have entire sections on these issues later. So, we have covered the theory around ingesting data. Although we will deal with some practical examples of this as well later in the web. We can also assume we have stored the data in a sensible way.

## Data processing

Moving on, let's talk about how we process the data that has been stored in order to do something useful with it. Recall the concept of ETL or extract, transform and load from our core concepts lecture. Traditionally, the process of ETL has been used for data ingestion to take data from a source, and then manipulate it to fit the parameters of the destination system. This might mean for example, the data needs to be changed to match a particular schema before it can be stored. 

With newer cloud-based systems for big data, ETL is sometimes now replaced with ELT, or extract, load and transform, where data can be loaded directly into a data lake and transformed later as required. In reality, you may end up using both types of approach or a compromise between the 2. There are some common transformations that you may apply to your data to make sure it is as valuable and as useful as possible. For example, formatting the data to match the destination storage system, labeling data perhaps with metadata regarding its source or other parameters, filtering out bad data, which could mean data that you don't want or need or data that has become corrupted during transit or validating the data to make sure that it meets certain other requirements before accepting it for storage. These are still high-level concepts, and you will see plenty of examples of these as you start to work with the GCP products and services that we explore here. But let's talk through just one example for now. Imagine you have a lamp connected to a smart home system. It may contain data such as this, it is ID, how brightly it is lit, the status of its bulb, maybe the local Wi-Fi hub it is connected to. We can format this data so that it fits the data storage system that we want to store this in. In this example, let's say it is a wide column NoSQL database. At the labeling stage, perhaps we add some metadata retrieved from another system based on the source of our lamp data to add our customer's ID. Next, we perform some internal filtering to remove the data we don't want or need. Perhaps we don't actually need the name of the customer's Wi-Fi network and storing it might be problematic. Finally, we can validate that this data is acceptable. Later, you will learn how transformations like these and others, can be achieved with Cloud Dataflow. Finally, once we have ingested, stored, and processed our data, it's time to do something with it. For most types of big data, this means some sort of process of visualization. 

## Data Visualization

Data is often visualized using various dashboard tools to provide live optics on the current state of an organization's metrics. In this example, we are looking at an AdWords report generated in Data Studio. Data Studio can also produce interactive reports that can be easily accessed by business analysts to quickly gain insights from data. Of course, visualization is just the end of one type of pipeline. Often, data is being used to provide decision-making for other parts of a system, perhaps for an e-commerce recommendation system, or maybe the data is being used to build a machine learning model, perhaps to do some classification or prediction work. As we work through the various technologies you need to learn for this blogs, you will see multiple examples of data pipelines and what they can produce. For now we have covered the major high-level concepts and we are ready to move on to the next page!